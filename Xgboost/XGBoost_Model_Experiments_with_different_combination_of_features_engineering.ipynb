{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_column', 150)\n",
    "\n",
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "# %%notify\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "Submission = pd.read_csv(\"sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.drop(\"revenue\",axis=1)\n",
    "train_y = pd.DataFrame(train[\"revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 42)\n",
      "(100000, 42)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100137, 42)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([train_x,test])\n",
    "print(data.shape)\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(df,column_name):\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    df = df.astype(float,errors=\"ignore\")\n",
    "    df[\"Open Date\"] = pd.to_datetime(df[\"Open Date\"], format='%m/%d/%Y')\n",
    "    df = df.set_index(\"Open Date\")\n",
    "    df[\"Open_Year\"] = df.index.year\n",
    "    df[\"Open_Month\"] = df.index.month\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = create_dummies(df,\"Type\")\n",
    "    df = create_dummies(df,\"City Group\")\n",
    "    df = create_dummies(df,\"City\")\n",
    "    df = create_dummies(df,\"Open_Year\")\n",
    "    df = create_dummies(df,\"Open_Month\")\n",
    "    df = df.drop([\"City\",\"City Group\",\"Type\",\"Open_Year\",\"Open_Month\"],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rf = feature_engineer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 139)\n",
      "(100000, 139)\n"
     ]
    }
   ],
   "source": [
    "train_rf_x = data_rf[:137]\n",
    "test_x = data_rf[137:]\n",
    "print(train_rf_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_x = train_rf_x.drop(\"Id\",axis=1)\n",
    "Submission_id = pd.DataFrame(test_x[\"Id\"]).astype(int).reset_index(drop=True)\n",
    "test_rf_x = test_x.drop(\"Id\",axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 138)\n",
      "(137, 1)\n",
      "(100000, 138)\n",
      "(100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_rf_x.shape) # Performed features normalization\n",
    "print(train_y.shape)\n",
    "print(test_rf_x.shape) # Performed features normalization\n",
    "print(Submission_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation for Train_y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_y_scaler = StandardScaler()\n",
    "normalize_train_y = pd.DataFrame(train_y_scaler.fit_transform(train_y),columns = train_y.columns)\n",
    "log_transform_y = train_y.apply(np.log)\n",
    "sqrt_log_y = train_y.apply(np.sqrt).apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Part_1: Drop Non-Mutual Features\n",
    "### As there are features available on test set only but not on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = (train_rf_x.sum()==0)\n",
    "drop_columns = drop_columns[drop_columns].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 105)\n",
      "(100000, 105)\n"
     ]
    }
   ],
   "source": [
    "train_rf_x_drop = train_rf_x.drop(drop_columns,axis=1)\n",
    "test_rf_x_drop = test_rf_x.drop(drop_columns,axis=1)\n",
    "print(train_rf_x_drop.shape)\n",
    "print(test_rf_x_drop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Part_2: Create N-way interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interaction features can be as 2-way interaction, 3 way or more by adjusting the polynomial degree\n",
    "n_way_interactions = PolynomialFeatures(2, interaction_only=True, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way_interactions_columns = train_rf_x_drop.select_dtypes(\"float\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions = pd.DataFrame(n_way_interactions.fit_transform(train_rf_x_drop[n_way_interactions_columns]),columns = n_way_interactions.get_feature_names(train_rf_x_drop[n_way_interactions_columns].columns))\n",
    "test_interactions = pd.DataFrame(n_way_interactions.fit_transform(test_rf_x_drop[n_way_interactions_columns]),columns = n_way_interactions.get_feature_names(test_rf_x_drop[n_way_interactions_columns].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_x_interaction = train_rf_x_drop.drop(n_way_interactions_columns,axis=1)\n",
    "test_rf_x_interaction = test_rf_x_drop.drop(n_way_interactions_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 771)\n",
      "(100000, 771)\n"
     ]
    }
   ],
   "source": [
    "train_rf_x_interaction = pd.concat([train_rf_x_interaction,train_interactions],axis=1)\n",
    "test_rf_x_interaction = pd.concat([test_rf_x_interaction,test_interactions],axis=1)\n",
    "print(train_rf_x_interaction.shape)\n",
    "print(test_rf_x_interaction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMax Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_columns = train_rf_x_interaction.select_dtypes(include=\"float\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Transform features only, and need no inverse_transform back\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train_score_scaler = MinMaxScaler()\n",
    "test_score_scaler = MinMaxScaler()\n",
    "normalize_train = pd.DataFrame(train_score_scaler.fit_transform(train_rf_x_interaction[min_max_columns]),columns = train_rf_x_interaction[min_max_columns].columns)\n",
    "normalize_test = pd.DataFrame(test_score_scaler.fit_transform(test_rf_x_interaction[min_max_columns]),columns = test_rf_x_interaction[min_max_columns].columns)\n",
    "# z- score transformation for train_rf_x & test_rf_x\n",
    "train_rf_x_interaction[min_max_columns] = normalize_train\n",
    "test_rf_x_interaction[min_max_columns] = normalize_test\n",
    "print(test_rf_x_interaction.isnull().sum().sum())\n",
    "print(train_rf_x_interaction[min_max_columns].equals(normalize_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Part_3: Remove Low Correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.concat([train_rf_x_interaction,log_transform_y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "# Sort the correlation values with the target columns revenue only\n",
    "features_train_revenue_corr = features_train.corr()['revenue'][:-1].abs().sort_values(ascending=False)\n",
    "revenue_corr_filter = features_train_revenue_corr[features_train_revenue_corr > 0.15]\n",
    "print(revenue_corr_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_corr_filter_columns = revenue_corr_filter.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 18)\n",
      "(100000, 18)\n"
     ]
    }
   ],
   "source": [
    "train_rf_x_engine = train_rf_x_interaction[revenue_corr_filter_columns]\n",
    "test_rf_x_engine = test_rf_x_interaction[revenue_corr_filter_columns]\n",
    "print(train_rf_x_engine.shape)\n",
    "print(test_rf_x_engine.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Part_4: PCA for dimensional reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train = PCA(n_components=31)\n",
    "pca_test = PCA(n_components=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_pca = pca_train.fit_transform(train_rf_x_engine)\n",
    "test_x_pca = pca_test.fit_transform(test_rf_x_engine)\n",
    "print(train_x_pca.shape)\n",
    "print(test_x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_train.explained_variance_ratio_.sum())\n",
    "print(pca_test.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_pca = pd.DataFrame(data = train_x_pca)\n",
    "test_x_pca = pd.DataFrame(data = test_x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_x_pca.shape)\n",
    "print(test_x_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Features collections to try in XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 18)\n",
      "(100000, 18)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_x_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b48a613232dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rf_x_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rf_x_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_transform_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x_pca' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_rf_x_engine.shape)\n",
    "print(test_rf_x_engine.shape)\n",
    "print(train_x_pca.shape)\n",
    "print(test_x_pca.shape)\n",
    "print(log_transform_y.shape)\n",
    "print(sqrt_log_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st: XGBoost, Grid Search with PCA Features with log_transformation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\"learning_rate\": [0.1,0.2,0.3],              # range: [0,1], default = 0.3\n",
    "                   # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "                   # The larger gamma is, the more conservative the algorithm will be.                   \n",
    "                   \"gamma\": [i/10.0 for i in range(1,10,2)],\n",
    "                   # Maximum depth of a tree. \n",
    "                   # Increasing this value will make the model more complex and more likely to overfit\n",
    "                   \"max_depth\": [6], \n",
    "                   # Used to control over-fitting\n",
    "                   # Too high values can lead to under-fitting hence, it should be tuned using grid search\n",
    "                   \"min_child_weight\": [1],\n",
    "                   # Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced\n",
    "                   \"max_delta_step\": [0],\n",
    "                   # Denotes the fraction of observations to be randomly samples for each tree\n",
    "                   # Typical values: 0.5-1\n",
    "                   \"subsample\": [0.8],\n",
    "                   # Similar to max_features in GBM, Typical values: 0.5 - 1\n",
    "                   # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "                   \"colsample_bytree\": [0.8,1],\n",
    "                   \"colsample_bylevel\": [1], # colsample_bytree will control the decision over this, default = 1\n",
    "                   \"colsample_bynode\": [1],  # colsample_bytree will control the decision over this, default = 1\n",
    "                   # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_lambda\": [1], # default = 1\n",
    "                   # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_alpha\": [1],  # default = 0\n",
    "                   # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "                   # A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "                   \"scale_pos_weight\": [1],\n",
    "                   # No. of trees ensemble, too high sometimes still can cause overfitting\n",
    "                   \"n_estimators\": [300,400], \n",
    "                   \"booster\": [\"gbtree\"],\n",
    "                   \"verbosity\": [1],\n",
    "                   \"objective\": [\"reg:squarederror\"],\n",
    "                   \"seed\": [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the iteration records for parameters tuning\n",
    "best_score_list = []\n",
    "best_params_list = []\n",
    "best_R2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_tuning = {'learning_rate': [0.1],               # 6th when boosting\n",
    "                          'gamma': [0.5],                       # 3rd\n",
    "                          'max_depth': [2],                     # 1st to tune\n",
    "                          'min_child_weight': [0.1],              # 2nd\n",
    "                          'max_delta_step': [0],\n",
    "                          'subsample': [0.6],                   # 4th\n",
    "                          'colsample_bytree': [0.8],            # 4th \n",
    "                          'colsample_bylevel': [1],\n",
    "                          'colsample_bynode': [1],\n",
    "                          'reg_lambda': [1],                    # 5th\n",
    "                          'reg_alpha': [0],                     # 5th\n",
    "                          'scale_pos_weight': [1.0],            # only when dealing with imbalance classes\n",
    "                          'n_estimators': [300],                # 1st\n",
    "                          \"booster\": [\"gbtree\"],\n",
    "                          \"verbosity\": [1],\n",
    "                          \"objective\": [\"reg:squarederror\"],\n",
    "                          \"seed\": [99]\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters setting inside Regressor\n",
    "xgboost = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = GridSearchCV(estimator = xgboost, param_grid = hyperparameters_tuning, cv = 10, iid = False, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.fit(train_x_pca,log_transform_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_score = xgboost_grid.cv_results_ #thus no need train-test split, as cv will automatic run for us \n",
    "best_params = xgboost_grid.best_params_\n",
    "best_score = xgboost_grid.best_score_\n",
    "best_rf = xgboost_grid.best_estimator_\n",
    "best_R2_score = best_rf.score(train_x_pca,log_transform_y)\n",
    "best_score_list.append(best_score)\n",
    "best_params_list.append(best_rf)\n",
    "best_R2_list.append(best_R2_score)\n",
    "print(best_score)\n",
    "print(best_R2_score)\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score_list)\n",
    "print(best_R2_list)\n",
    "print(best_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost's DMatrix, after fine tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = train_x_pca, label = log_transform_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the learning_rate and set a large num_boost_round hyperparameter to ensure convergence. \n",
    "# If convergence is slow, retry with a slightly higher learning rate (e.g. 0.075 instead of 0.05)\n",
    "num_boost_round = 150000\n",
    "early_stopping_rounds = 50\n",
    "# Activates early stopping. CV error needs to decrease at least every <early_stopping_rounds> round(s) to continue.\n",
    "# Last entry in evaluation history is the one from best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_boosting ={'learning_rate': 0.001,               # 6th when boosting\n",
    "                          'gamma': 0.5,                       # 3rd\n",
    "                          'max_depth': 2,                     # 1st to tune\n",
    "                          'min_child_weight': 0.1,              # 2nd\n",
    "                          'max_delta_step': 0,\n",
    "                          'subsample': 0.6,                   # 4th\n",
    "                          'colsample_bytree': 0.8,            # 4th \n",
    "                          'colsample_bylevel': 1,\n",
    "                          'colsample_bynode': 1,\n",
    "                          'reg_lambda': 1,                    # 5th\n",
    "                          'reg_alpha': 0,                     # 5th\n",
    "                          'scale_pos_weight': 1.0,            # only when dealing with imbalance classes\n",
    "                          'n_estimators': 300,                # 1st\n",
    "                          \"booster\": \"gbtree\",\n",
    "                          \"verbosity\": 1,\n",
    "                          \"objective\": \"reg:squarederror\",\n",
    "                          \"seed\": 99\n",
    "                         }                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbCV = xgb.cv(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 10, #same as CV\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalise XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "xgb.plot_importance(xgbFinal, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = xgbFinal.predict(xgb.DMatrix(test_x_pca))\n",
    "xgbFinal_submission = np.exp(xgbFinal_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other submission style\n",
    "## Creating a Submission File to submit to Kaggle competition ##\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": testData[\"Id\"],\n",
    "        \"Prediction\": xgbFinal_submission\n",
    "    })\n",
    "submission.to_csv('Best_model_5th(0.001)_trial.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = pickle.load(open(\"xgbFinal.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-forecast the prediction to verify the model\n",
    "xgb_test_p = xgb_test.predict(xgb.DMatrix(test_rf_x))\n",
    "xgb_test_p = np.exp(xgb_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = (xgbFinal_submission == xgb_test_p)\n",
    "verify_model.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd: XGBoost, Grid Search with PCA Features with sqrt_log_transformation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\"learning_rate\": [0.1,0.2,0.3],              # range: [0,1], default = 0.3\n",
    "                   # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "                   # The larger gamma is, the more conservative the algorithm will be.                   \n",
    "                   \"gamma\": [i/10.0 for i in range(1,10,2)],\n",
    "                   # Maximum depth of a tree. \n",
    "                   # Increasing this value will make the model more complex and more likely to overfit\n",
    "                   \"max_depth\": [6], \n",
    "                   # Used to control over-fitting\n",
    "                   # Too high values can lead to under-fitting hence, it should be tuned using grid search\n",
    "                   \"min_child_weight\": [1],\n",
    "                   # Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced\n",
    "                   \"max_delta_step\": [0],\n",
    "                   # Denotes the fraction of observations to be randomly samples for each tree\n",
    "                   # Typical values: 0.5-1\n",
    "                   \"subsample\": [0.8],\n",
    "                   # Similar to max_features in GBM, Typical values: 0.5 - 1\n",
    "                   # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "                   \"colsample_bytree\": [0.8,1],\n",
    "                   \"colsample_bylevel\": [1], # colsample_bytree will control the decision over this, default = 1\n",
    "                   \"colsample_bynode\": [1],  # colsample_bytree will control the decision over this, default = 1\n",
    "                   # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_lambda\": [1], # default = 1\n",
    "                   # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_alpha\": [1],  # default = 0\n",
    "                   # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "                   # A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "                   \"scale_pos_weight\": [1],\n",
    "                   # No. of trees ensemble, too high sometimes still can cause overfitting\n",
    "                   \"n_estimators\": [300,400], \n",
    "                   \"booster\": [\"gbtree\"],\n",
    "                   \"verbosity\": [1],\n",
    "                   \"objective\": [\"reg:squarederror\"],\n",
    "                   \"seed\": [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the iteration records for parameters tuning\n",
    "best_score_list = []\n",
    "best_params_list = []\n",
    "best_R2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_tuning = {'learning_rate': [0.1],               # 6th when boosting\n",
    "                          'gamma': [0.1],                       # 3rd\n",
    "                          'max_depth': [3],                     # 1st to tune\n",
    "                          'min_child_weight': [0.1],              # 2nd\n",
    "                          'max_delta_step': [0],\n",
    "                          'subsample': [0.7],                   # 4th\n",
    "                          'colsample_bytree': [0.7],            # 4th \n",
    "                          'colsample_bylevel': [1],\n",
    "                          'colsample_bynode': [1],\n",
    "                          'reg_lambda': [1],                    # 5th\n",
    "                          'reg_alpha': [0],                     # 5th\n",
    "                          'scale_pos_weight': [1.0],            # only when dealing with imbalance classes\n",
    "                          'n_estimators': [260],                # 1st\n",
    "                          \"booster\": [\"gbtree\"],\n",
    "                          \"verbosity\": [1],\n",
    "                          \"objective\": [\"reg:squarederror\"],\n",
    "                          \"seed\": [99]\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters setting inside Regressor\n",
    "xgboost = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = GridSearchCV(estimator = xgboost, param_grid = hyperparameters_tuning, cv = 10, iid = False, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.fit(train_x_pca,sqrt_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_score = xgboost_grid.cv_results_ #thus no need train-test split, as cv will automatic run for us \n",
    "best_params = xgboost_grid.best_params_\n",
    "best_score = xgboost_grid.best_score_\n",
    "best_rf = xgboost_grid.best_estimator_\n",
    "best_R2_score = best_rf.score(train_x_pca,sqrt_log_y)\n",
    "best_score_list.append(best_score)\n",
    "best_params_list.append(best_rf)\n",
    "best_R2_list.append(best_R2_score)\n",
    "print(best_score)\n",
    "print(best_R2_score)\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score_list)\n",
    "print(best_R2_list)\n",
    "print(best_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost's DMatrix, after fine tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = train_x_pca, label = sqrt_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the learning_rate and set a large num_boost_round hyperparameter to ensure convergence. \n",
    "# If convergence is slow, retry with a slightly higher learning rate (e.g. 0.075 instead of 0.05)\n",
    "num_boost_round = 150000\n",
    "early_stopping_rounds = 50\n",
    "# Activates early stopping. CV error needs to decrease at least every <early_stopping_rounds> round(s) to continue.\n",
    "# Last entry in evaluation history is the one from best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_boosting ={'learning_rate': 0.001,               # 6th when boosting\n",
    "                          'gamma': 0.1,                       # 3rd\n",
    "                          'max_depth': 3,                     # 1st to tune\n",
    "                          'min_child_weight': 0.1,              # 2nd\n",
    "                          'max_delta_step': 0,\n",
    "                          'subsample': 0.7,                   # 4th\n",
    "                          'colsample_bytree': 0.7,            # 4th \n",
    "                          'colsample_bylevel': 1,\n",
    "                          'colsample_bynode': 1,\n",
    "                          'reg_lambda': 1,                    # 5th\n",
    "                          'reg_alpha': 0,                     # 5th\n",
    "                          'scale_pos_weight': 1.0,            # only when dealing with imbalance classes\n",
    "                          'n_estimators': 260,                # 1st\n",
    "                          \"booster\": \"gbtree\",\n",
    "                          \"verbosity\": 1,\n",
    "                          \"objective\": \"reg:squarederror\",\n",
    "                          \"seed\": 99\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbCV = xgb.cv(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 10, #same as CV\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalise XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "xgb.plot_importance(xgbFinal, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = xgbFinal.predict(xgb.DMatrix(test_x_pca))\n",
    "xgbFinal_submission = np.square(np.exp(xgbFinal_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other submission style\n",
    "## Creating a Submission File to submit to Kaggle competition ##\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": testData[\"Id\"],\n",
    "        \"Prediction\": xgbFinal_submission\n",
    "    })\n",
    "submission.to_csv('Best_model_6th(0.001)_trial.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = pickle.load(open(\"xgbFinal.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-forecast the prediction to verify the model\n",
    "xgb_test_p = xgb_test.predict(xgb.DMatrix(test_rf_x))\n",
    "xgb_test_p = np.exp(xgb_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = (xgbFinal_submission == xgb_test_p)\n",
    "verify_model.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd: XGBoost, Grid Search with Interaction & High Correlation(>0.10) Features only with log_transformation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\"learning_rate\": [0.1,0.2,0.3],              # range: [0,1], default = 0.3\n",
    "                   # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "                   # The larger gamma is, the more conservative the algorithm will be.                   \n",
    "                   \"gamma\": [i/10.0 for i in range(1,10,2)],\n",
    "                   # Maximum depth of a tree. \n",
    "                   # Increasing this value will make the model more complex and more likely to overfit\n",
    "                   \"max_depth\": [6], \n",
    "                   # Used to control over-fitting\n",
    "                   # Too high values can lead to under-fitting hence, it should be tuned using grid search\n",
    "                   \"min_child_weight\": [1],\n",
    "                   # Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced\n",
    "                   \"max_delta_step\": [0],\n",
    "                   # Denotes the fraction of observations to be randomly samples for each tree\n",
    "                   # Typical values: 0.5-1\n",
    "                   \"subsample\": [0.8],\n",
    "                   # Similar to max_features in GBM, Typical values: 0.5 - 1\n",
    "                   # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "                   \"colsample_bytree\": [0.8,1],\n",
    "                   \"colsample_bylevel\": [1], # colsample_bytree will control the decision over this, default = 1\n",
    "                   \"colsample_bynode\": [1],  # colsample_bytree will control the decision over this, default = 1\n",
    "                   # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_lambda\": [1], # default = 1\n",
    "                   # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_alpha\": [1],  # default = 0\n",
    "                   # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "                   # A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "                   \"scale_pos_weight\": [1],\n",
    "                   # No. of trees ensemble, too high sometimes still can cause overfitting\n",
    "                   \"n_estimators\": [300,400], \n",
    "                   \"booster\": [\"gbtree\"],\n",
    "                   \"verbosity\": [1],\n",
    "                   \"objective\": [\"reg:squarederror\"],\n",
    "                   \"seed\": [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the iteration records for parameters tuning\n",
    "best_score_list = []\n",
    "best_params_list = []\n",
    "best_R2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_tuning = {'learning_rate': [0.01],               # 6th when boosting\n",
    "                          'gamma': [0.1],                       # 3rd\n",
    "                          'max_depth': [3],                     # 1st to tune\n",
    "                          'min_child_weight': [0.1],              # 2nd\n",
    "                          'max_delta_step': [0],\n",
    "                          'subsample': [0.2],                   # 4th\n",
    "                          'colsample_bytree': [0.8],            # 4th \n",
    "                          'colsample_bylevel': [1],\n",
    "                          'colsample_bynode': [1],\n",
    "                          'reg_lambda': [1],                    # 5th\n",
    "                          'reg_alpha': [0],                     # 5th\n",
    "                          'scale_pos_weight': [1.0],            # only when dealing with imbalance classes\n",
    "                          'n_estimators': [710],                # 1st\n",
    "                          \"booster\": [\"gbtree\"],\n",
    "                          \"verbosity\": [1],\n",
    "                          \"objective\": [\"reg:squarederror\"],\n",
    "                          \"seed\": [99]\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters setting inside Regressor\n",
    "xgboost = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = GridSearchCV(estimator = xgboost, param_grid = hyperparameters_tuning, cv = 10, iid = False, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.fit(train_rf_x_engine,log_transform_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_score = xgboost_grid.cv_results_ #thus no need train-test split, as cv will automatic run for us \n",
    "best_params = xgboost_grid.best_params_\n",
    "best_score = xgboost_grid.best_score_\n",
    "best_rf = xgboost_grid.best_estimator_\n",
    "best_R2_score = best_rf.score(train_rf_x_engine,log_transform_y)\n",
    "best_score_list.append(best_score)\n",
    "best_params_list.append(best_rf)\n",
    "best_R2_list.append(best_R2_score)\n",
    "print(best_score)\n",
    "print(best_R2_score)\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score_list)\n",
    "print(best_R2_list)\n",
    "print(best_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost's DMatrix, after fine tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = train_rf_x_engine, label = log_transform_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the learning_rate and set a large num_boost_round hyperparameter to ensure convergence. \n",
    "# If convergence is slow, retry with a slightly higher learning rate (e.g. 0.075 instead of 0.05)\n",
    "num_boost_round = 150000\n",
    "early_stopping_rounds = 50\n",
    "# Activates early stopping. CV error needs to decrease at least every <early_stopping_rounds> round(s) to continue.\n",
    "# Last entry in evaluation history is the one from best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_boosting = {'learning_rate': 0.001,               # 6th when boosting\n",
    "                          'gamma': 0.1,                       # 3rd\n",
    "                          'max_depth': 3,                     # 1st to tune\n",
    "                          'min_child_weight': 0.1,              # 2nd\n",
    "                          'max_delta_step': 0,\n",
    "                          'subsample': 0.2,                   # 4th\n",
    "                          'colsample_bytree': 0.8,            # 4th \n",
    "                          'colsample_bylevel': 1,\n",
    "                          'colsample_bynode': 1,\n",
    "                          'reg_lambda': 1,                    # 5th\n",
    "                          'reg_alpha': 0,                     # 5th\n",
    "                          'scale_pos_weight': 1.0,            # only when dealing with imbalance classes\n",
    "                          'n_estimators': 710,                # 1st\n",
    "                          \"booster\": \"gbtree\",\n",
    "                          \"verbosity\": 1,\n",
    "                          \"objective\": \"reg:squarederror\",\n",
    "                          \"seed\": 99\n",
    "                         }                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbCV = xgb.cv(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 10, #same as CV\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalise XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "xgb.plot_importance(xgbFinal, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = best_rf.predict(test_rf_x_engine)\n",
    "xgbFinal_submission = np.exp(xgbFinal_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = xgbFinal.predict(xgb.DMatrix(test_rf_x_engine))\n",
    "xgbFinal_submission = np.exp(xgbFinal_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other submission style\n",
    "## Creating a Submission File to submit to Kaggle competition ##\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": testData[\"Id\"],\n",
    "        \"Prediction\": xgbFinal_submission\n",
    "    })\n",
    "submission.to_csv('Best_model_8th(0.001)_trial.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = pickle.load(open(\"xgbFinal.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-forecast the prediction to verify the model\n",
    "xgb_test_p = xgb_test.predict(xgb.DMatrix(test_rf_x))\n",
    "xgb_test_p = np.exp(xgb_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = (xgbFinal_submission == xgb_test_p)\n",
    "verify_model.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th, XGBoost, Grid Search with Mutual Features & High Correlation(0.10) with sqrt_log_transformation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\"learning_rate\": [0.1,0.2,0.3],              # range: [0,1], default = 0.3\n",
    "                   # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "                   # The larger gamma is, the more conservative the algorithm will be.                   \n",
    "                   \"gamma\": [i/10.0 for i in range(1,10,2)],\n",
    "                   # Maximum depth of a tree. \n",
    "                   # Increasing this value will make the model more complex and more likely to overfit\n",
    "                   \"max_depth\": [6], \n",
    "                   # Used to control over-fitting\n",
    "                   # Too high values can lead to under-fitting hence, it should be tuned using grid search\n",
    "                   \"min_child_weight\": [1],\n",
    "                   # Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced\n",
    "                   \"max_delta_step\": [0],\n",
    "                   # Denotes the fraction of observations to be randomly samples for each tree\n",
    "                   # Typical values: 0.5-1\n",
    "                   \"subsample\": [0.8],\n",
    "                   # Similar to max_features in GBM, Typical values: 0.5 - 1\n",
    "                   # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "                   \"colsample_bytree\": [0.8,1],\n",
    "                   \"colsample_bylevel\": [1], # colsample_bytree will control the decision over this, default = 1\n",
    "                   \"colsample_bynode\": [1],  # colsample_bytree will control the decision over this, default = 1\n",
    "                   # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_lambda\": [1], # default = 1\n",
    "                   # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_alpha\": [1],  # default = 0\n",
    "                   # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "                   # A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "                   \"scale_pos_weight\": [1],\n",
    "                   # No. of trees ensemble, too high sometimes still can cause overfitting\n",
    "                   \"n_estimators\": [300,400], \n",
    "                   \"booster\": [\"gbtree\"],\n",
    "                   \"verbosity\": [1],\n",
    "                   \"objective\": [\"reg:squarederror\"],\n",
    "                   \"seed\": [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the iteration records for parameters tuning\n",
    "best_score_list = []\n",
    "best_params_list = []\n",
    "best_R2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_tuning = {'learning_rate': [0.05],               # 6th when boosting\n",
    "                          'gamma': [0.1],                       # 3rd\n",
    "                          'max_depth': [3],                     # 1st to tune\n",
    "                          'min_child_weight': [0.1],              # 2nd\n",
    "                          'max_delta_step': [0],\n",
    "                          'subsample': [0.9],                   # 4th\n",
    "                          'colsample_bytree': [0.2],            # 4th \n",
    "                          'colsample_bylevel': [1],\n",
    "                          'colsample_bynode': [1],\n",
    "                          'reg_lambda': [0],                    # 5th\n",
    "                          'reg_alpha': [0],                     # 5th\n",
    "                          'scale_pos_weight': [1.0],            # only when dealing with imbalance classes\n",
    "                          'n_estimators': [130],                # 1st\n",
    "                          \"booster\": [\"gbtree\"],\n",
    "                          \"verbosity\": [1],\n",
    "                          \"objective\": [\"reg:squarederror\"],\n",
    "                          \"seed\": [99]\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters setting inside Regressor\n",
    "xgboost = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = GridSearchCV(estimator = xgboost, param_grid = hyperparameters_tuning, cv = 10, iid = False, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.fit(train_rf_x_engine,sqrt_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_score = xgboost_grid.cv_results_ #thus no need train-test split, as cv will automatic run for us \n",
    "best_params = xgboost_grid.best_params_\n",
    "best_score = xgboost_grid.best_score_\n",
    "best_rf = xgboost_grid.best_estimator_\n",
    "best_R2_score = best_rf.score(train_rf_x_engine,sqrt_log_y)\n",
    "best_score_list.append(best_score)\n",
    "best_params_list.append(best_rf)\n",
    "best_R2_list.append(best_R2_score)\n",
    "print(best_score)\n",
    "print(best_R2_score)\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score_list)\n",
    "print(best_R2_list)\n",
    "print(best_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost's DMatrix, after fine tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = train_rf_x_engine, label = sqrt_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the learning_rate and set a large num_boost_round hyperparameter to ensure convergence. \n",
    "# If convergence is slow, retry with a slightly higher learning rate (e.g. 0.075 instead of 0.05)\n",
    "num_boost_round = 150000\n",
    "early_stopping_rounds = 50\n",
    "# Activates early stopping. CV error needs to decrease at least every <early_stopping_rounds> round(s) to continue.\n",
    "# Last entry in evaluation history is the one from best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_boosting = {'learning_rate': 0.001,               # 6th when boosting\n",
    "                          'gamma': 0.1,                       # 3rd\n",
    "                          'max_depth': 3,                     # 1st to tune\n",
    "                          'min_child_weight': 0.1,              # 2nd\n",
    "                          'max_delta_step': 0,\n",
    "                          'subsample': 0.9,                   # 4th\n",
    "                          'colsample_bytree': 0.2,            # 4th \n",
    "                          'colsample_bylevel': 1,\n",
    "                          'colsample_bynode': 1,\n",
    "                          'reg_lambda': 0,                    # 5th\n",
    "                          'reg_alpha': 0,                     # 5th\n",
    "                          'scale_pos_weight': 1.0,            # only when dealing with imbalance classes\n",
    "                          'n_estimators': 130,                # 1st\n",
    "                          \"booster\": \"gbtree\",\n",
    "                          \"verbosity\": 1,\n",
    "                          \"objective\": \"reg:squarederror\",\n",
    "                          \"seed\": 99\n",
    "                         }                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbCV = xgb.cv(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 10, #same as CV\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalise XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "xgb.plot_importance(xgbFinal, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = xgbFinal.predict(xgb.DMatrix(test_rf_x_engine))\n",
    "xgbFinal_submission = np.square(np.exp(xgbFinal_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other submission style\n",
    "## Creating a Submission File to submit to Kaggle competition ##\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": testData[\"Id\"],\n",
    "        \"Prediction\": xgbFinal_submission\n",
    "    })\n",
    "submission.to_csv('Best_model_1(0.001)_trial.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = pickle.load(open(\"xgbFinal.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-forecast the prediction to verify the model\n",
    "xgb_test_p = xgb_test.predict(xgb.DMatrix(test_rf_x_engine))\n",
    "xgb_test_p = np.exp(xgb_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = (xgbFinal_submission == xgb_test_p)\n",
    "verify_model.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th: XGBoost, Grid Search with Interaction & High Correlation(>0.15) Features only with log_transformation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\"learning_rate\": [0.1,0.2,0.3],              # range: [0,1], default = 0.3\n",
    "                   # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "                   # The larger gamma is, the more conservative the algorithm will be.                   \n",
    "                   \"gamma\": [i/10.0 for i in range(1,10,2)],\n",
    "                   # Maximum depth of a tree. \n",
    "                   # Increasing this value will make the model more complex and more likely to overfit\n",
    "                   \"max_depth\": [6], \n",
    "                   # Used to control over-fitting\n",
    "                   # Too high values can lead to under-fitting hence, it should be tuned using grid search\n",
    "                   \"min_child_weight\": [1],\n",
    "                   # Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced\n",
    "                   \"max_delta_step\": [0],\n",
    "                   # Denotes the fraction of observations to be randomly samples for each tree\n",
    "                   # Typical values: 0.5-1\n",
    "                   \"subsample\": [0.8],\n",
    "                   # Similar to max_features in GBM, Typical values: 0.5 - 1\n",
    "                   # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "                   \"colsample_bytree\": [0.8,1],\n",
    "                   \"colsample_bylevel\": [1], # colsample_bytree will control the decision over this, default = 1\n",
    "                   \"colsample_bynode\": [1],  # colsample_bytree will control the decision over this, default = 1\n",
    "                   # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_lambda\": [1], # default = 1\n",
    "                   # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_alpha\": [1],  # default = 0\n",
    "                   # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "                   # A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "                   \"scale_pos_weight\": [1],\n",
    "                   # No. of trees ensemble, too high sometimes still can cause overfitting\n",
    "                   \"n_estimators\": [300,400], \n",
    "                   \"booster\": [\"gbtree\"],\n",
    "                   \"verbosity\": [1],\n",
    "                   \"objective\": [\"reg:squarederror\"],\n",
    "                   \"seed\": [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the iteration records for parameters tuning\n",
    "best_score_list = []\n",
    "best_params_list = []\n",
    "best_R2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_tuning = {'learning_rate': [0.05],               # 6th when boosting\n",
    "                          'gamma': [0],                       # 3rd\n",
    "                          'max_depth': [2],                     # 1st to tune\n",
    "                          'min_child_weight': [0.1],              # 2nd\n",
    "                          'max_delta_step': [0],\n",
    "                          'subsample': [0.9],                   # 4th\n",
    "                          'colsample_bytree': [0.1],            # 4th \n",
    "                          'colsample_bylevel': [1],\n",
    "                          'colsample_bynode': [1],\n",
    "                          'reg_lambda': [0],                    # 5th\n",
    "                          'reg_alpha': [0],                     # 5th\n",
    "                          'scale_pos_weight': [1.0],            # only when dealing with imbalance classes\n",
    "                          'n_estimators': [150],                # 1st\n",
    "                          \"booster\": [\"gbtree\"],\n",
    "                          \"verbosity\": [1],\n",
    "                          \"objective\": [\"reg:squarederror\"],\n",
    "                          \"seed\": [99]\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters setting inside Regressor\n",
    "xgboost = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = GridSearchCV(estimator = xgboost, param_grid = hyperparameters_tuning, cv = 10, iid = False, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'learning_rate': [0.05], 'gamma': [0], 'max_depth': [2], 'min_child_weight': [0.1], 'max_delta_step': [0], 'subsample': [0.9], 'colsample_bytree': [0.1], 'colsample_bylevel': [1], 'colsample_bynode': [1], 'reg_lambda': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9], 'reg_alpha': [0], 'scale_pos_weight': [1.0], 'n_estimators': [150], 'booster': ['gbtree'], 'verbosity': [1], 'objective': ['reg:squarederror'], 'seed': [99]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_grid.fit(train_rf_x_engine,log_transform_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1939798893670266\n",
      "0.30542657219387714\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=0.1, missing=None, n_estimators=150,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0.7, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.9, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "best_cv_score = xgboost_grid.cv_results_ #thus no need train-test split, as cv will automatic run for us \n",
    "best_params = xgboost_grid.best_params_\n",
    "best_score = xgboost_grid.best_score_\n",
    "best_rf = xgboost_grid.best_estimator_\n",
    "best_R2_score = best_rf.score(train_rf_x_engine,log_transform_y)\n",
    "best_score_list.append(best_score)\n",
    "best_params_list.append(best_rf)\n",
    "best_R2_list.append(best_R2_score)\n",
    "print(best_score)\n",
    "print(best_R2_score)\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3811379085095802, -0.1984436411533798, -0.19418048634820112, -0.2071351983017649, -0.19378022577636905, -0.19786000396856168, -0.19378022577636905, -0.1939798893670266]\n",
      "[-0.4067121643828935, 0.43976002102661993, 0.39965794303201907, 0.4516438269645361, 0.38569817847278554, 0.37474816732229665, 0.38569817847278554, 0.30542657219387714]\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.2, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=0, missing=None, n_estimators=350,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.6, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.2, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=1, min_child_weight=0, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.6, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.2, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=1, min_child_weight=0, missing=None, n_estimators=150,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.7, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.2, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=1, min_child_weight=0, missing=None, n_estimators=50,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.7, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=0, missing=None, n_estimators=150,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.9, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.1, gamma=0.1,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=0, missing=None, n_estimators=150,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.9, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=0.1, missing=None, n_estimators=150,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.9, verbosity=1), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=0.1, missing=None, n_estimators=150,\n",
      "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=0.7, scale_pos_weight=1.0,\n",
      "       seed=99, silent=None, subsample=0.9, verbosity=1)]\n"
     ]
    }
   ],
   "source": [
    "print(best_score_list)\n",
    "print(best_R2_list)\n",
    "print(best_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost's DMatrix, after fine tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = train_rf_x_engine, label = log_transform_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the learning_rate and set a large num_boost_round hyperparameter to ensure convergence. \n",
    "# If convergence is slow, retry with a slightly higher learning rate (e.g. 0.075 instead of 0.05)\n",
    "num_boost_round = 150000\n",
    "early_stopping_rounds = 50\n",
    "# Activates early stopping. CV error needs to decrease at least every <early_stopping_rounds> round(s) to continue.\n",
    "# Last entry in evaluation history is the one from best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_boosting ={'learning_rate': 0.05,               # 6th when boosting\n",
    "                          'gamma': 0,                       # 3rd\n",
    "                          'max_depth': 2,                     # 1st to tune\n",
    "                          'min_child_weight': 0.1,              # 2nd\n",
    "                          'max_delta_step': 0,\n",
    "                          'subsample': 0.9,                   # 4th\n",
    "                          'colsample_bytree': 0.1,            # 4th \n",
    "                          'colsample_bylevel': 1,\n",
    "                          'colsample_bynode': 1,\n",
    "                          'reg_lambda': 0,                    # 5th\n",
    "                          'reg_alpha': 0,                     # 5th\n",
    "                          'scale_pos_weight': 1.0,            # only when dealing with imbalance classes\n",
    "                          'n_estimators': 150,                # 1st\n",
    "                          \"booster\": \"gbtree\",\n",
    "                          \"verbosity\": 1,\n",
    "                          \"objective\": \"reg:squarederror\",\n",
    "                          \"seed\": 99\n",
    "                         }                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:13.9605+0.0134369\ttest-rmse:13.9588+0.12786\n",
      "[1]\ttrain-rmse:13.2631+0.0128132\ttest-rmse:13.2616+0.127244\n",
      "[2]\ttrain-rmse:12.6009+0.0121477\ttest-rmse:12.599+0.12729\n",
      "[3]\ttrain-rmse:11.9714+0.0113061\ttest-rmse:11.969+0.127545\n",
      "[4]\ttrain-rmse:11.374+0.0107412\ttest-rmse:11.3702+0.129018\n",
      "[5]\ttrain-rmse:10.8062+0.0106731\ttest-rmse:10.803+0.129384\n",
      "[6]\ttrain-rmse:10.267+0.0101941\ttest-rmse:10.2637+0.129348\n",
      "[7]\ttrain-rmse:9.75435+0.00965762\ttest-rmse:9.75128+0.129239\n",
      "[8]\ttrain-rmse:9.26767+0.00915939\ttest-rmse:9.26473+0.129428\n",
      "[9]\ttrain-rmse:8.80524+0.00901517\ttest-rmse:8.80247+0.12944\n",
      "[10]\ttrain-rmse:8.36604+0.00853326\ttest-rmse:8.36278+0.130606\n",
      "[11]\ttrain-rmse:7.94896+0.00829499\ttest-rmse:7.94618+0.131195\n",
      "[12]\ttrain-rmse:7.55252+0.00783757\ttest-rmse:7.54949+0.13116\n",
      "[13]\ttrain-rmse:7.17644+0.00784431\ttest-rmse:7.17305+0.130483\n",
      "[14]\ttrain-rmse:6.81864+0.00726773\ttest-rmse:6.81495+0.130119\n",
      "[15]\ttrain-rmse:6.47943+0.00674688\ttest-rmse:6.47585+0.13044\n",
      "[16]\ttrain-rmse:6.1569+0.0064242\ttest-rmse:6.15338+0.129528\n",
      "[17]\ttrain-rmse:5.85043+0.0059993\ttest-rmse:5.84649+0.129698\n",
      "[18]\ttrain-rmse:5.55953+0.00610882\ttest-rmse:5.55501+0.128892\n",
      "[19]\ttrain-rmse:5.2832+0.00581876\ttest-rmse:5.27878+0.128745\n",
      "[20]\ttrain-rmse:5.02097+0.00523137\ttest-rmse:5.01709+0.128919\n",
      "[21]\ttrain-rmse:4.77178+0.0047952\ttest-rmse:4.76808+0.129098\n",
      "[22]\ttrain-rmse:4.53513+0.00454939\ttest-rmse:4.53178+0.129404\n",
      "[23]\ttrain-rmse:4.31066+0.00439968\ttest-rmse:4.30739+0.128853\n",
      "[24]\ttrain-rmse:4.09766+0.00411695\ttest-rmse:4.09424+0.129282\n",
      "[25]\ttrain-rmse:3.89492+0.00395157\ttest-rmse:3.89161+0.129046\n",
      "[26]\ttrain-rmse:3.70256+0.00352066\ttest-rmse:3.69831+0.129668\n",
      "[27]\ttrain-rmse:3.51988+0.00382815\ttest-rmse:3.51629+0.128716\n",
      "[28]\ttrain-rmse:3.34648+0.00344915\ttest-rmse:3.34354+0.128399\n",
      "[29]\ttrain-rmse:3.18164+0.00363617\ttest-rmse:3.17826+0.128398\n",
      "[30]\ttrain-rmse:3.02547+0.00364974\ttest-rmse:3.02231+0.128611\n",
      "[31]\ttrain-rmse:2.87724+0.00378389\ttest-rmse:2.87345+0.128679\n",
      "[32]\ttrain-rmse:2.73583+0.00416815\ttest-rmse:2.73178+0.127626\n",
      "[33]\ttrain-rmse:2.60251+0.00389626\ttest-rmse:2.59899+0.127678\n",
      "[34]\ttrain-rmse:2.47565+0.00397047\ttest-rmse:2.47302+0.127283\n",
      "[35]\ttrain-rmse:2.35548+0.0036926\ttest-rmse:2.35208+0.128078\n",
      "[36]\ttrain-rmse:2.24151+0.00342504\ttest-rmse:2.23776+0.128013\n",
      "[37]\ttrain-rmse:2.13355+0.00325629\ttest-rmse:2.12975+0.127427\n",
      "[38]\ttrain-rmse:2.03128+0.00302402\ttest-rmse:2.0274+0.127632\n",
      "[39]\ttrain-rmse:1.93406+0.00314059\ttest-rmse:1.93066+0.127099\n",
      "[40]\ttrain-rmse:1.84213+0.00324443\ttest-rmse:1.83873+0.126947\n",
      "[41]\ttrain-rmse:1.75478+0.00269687\ttest-rmse:1.75097+0.126899\n",
      "[42]\ttrain-rmse:1.67221+0.0026211\ttest-rmse:1.66901+0.127001\n",
      "[43]\ttrain-rmse:1.59424+0.00276131\ttest-rmse:1.59066+0.12696\n",
      "[44]\ttrain-rmse:1.5203+0.0032286\ttest-rmse:1.51594+0.126578\n",
      "[45]\ttrain-rmse:1.45037+0.00327547\ttest-rmse:1.44647+0.126068\n",
      "[46]\ttrain-rmse:1.38407+0.00363048\ttest-rmse:1.38+0.125937\n",
      "[47]\ttrain-rmse:1.32137+0.00372604\ttest-rmse:1.31809+0.125567\n",
      "[48]\ttrain-rmse:1.26189+0.00386128\ttest-rmse:1.25914+0.125489\n",
      "[49]\ttrain-rmse:1.20596+0.00373342\ttest-rmse:1.20349+0.125363\n",
      "[50]\ttrain-rmse:1.15318+0.00366916\ttest-rmse:1.15077+0.12497\n",
      "[51]\ttrain-rmse:1.10307+0.00381129\ttest-rmse:1.10101+0.12476\n",
      "[52]\ttrain-rmse:1.05606+0.00411759\ttest-rmse:1.0547+0.123741\n",
      "[53]\ttrain-rmse:1.01152+0.00389467\ttest-rmse:1.01095+0.123805\n",
      "[54]\ttrain-rmse:0.969417+0.00426249\ttest-rmse:0.96942+0.122782\n",
      "[55]\ttrain-rmse:0.929532+0.00416787\ttest-rmse:0.92976+0.122464\n",
      "[56]\ttrain-rmse:0.892454+0.00418613\ttest-rmse:0.893283+0.121397\n",
      "[57]\ttrain-rmse:0.857349+0.00434612\ttest-rmse:0.859111+0.120567\n",
      "[58]\ttrain-rmse:0.824448+0.00455764\ttest-rmse:0.827117+0.120314\n",
      "[59]\ttrain-rmse:0.793585+0.0045741\ttest-rmse:0.796691+0.119606\n",
      "[60]\ttrain-rmse:0.764103+0.00494439\ttest-rmse:0.768251+0.118693\n",
      "[61]\ttrain-rmse:0.736965+0.00490246\ttest-rmse:0.742533+0.118262\n",
      "[62]\ttrain-rmse:0.711524+0.00528912\ttest-rmse:0.717572+0.117359\n",
      "[63]\ttrain-rmse:0.687741+0.00569726\ttest-rmse:0.6949+0.117202\n",
      "[64]\ttrain-rmse:0.665269+0.00568848\ttest-rmse:0.674118+0.116185\n",
      "[65]\ttrain-rmse:0.644294+0.00613538\ttest-rmse:0.654079+0.114973\n",
      "[66]\ttrain-rmse:0.624904+0.00625569\ttest-rmse:0.635692+0.114345\n",
      "[67]\ttrain-rmse:0.606688+0.00635681\ttest-rmse:0.618418+0.113193\n",
      "[68]\ttrain-rmse:0.589932+0.0064256\ttest-rmse:0.602808+0.11254\n",
      "[69]\ttrain-rmse:0.574104+0.00656453\ttest-rmse:0.588036+0.111409\n",
      "[70]\ttrain-rmse:0.559883+0.00650628\ttest-rmse:0.574605+0.110324\n",
      "[71]\ttrain-rmse:0.54639+0.00685316\ttest-rmse:0.562097+0.108989\n",
      "[72]\ttrain-rmse:0.533911+0.00688124\ttest-rmse:0.550246+0.107796\n",
      "[73]\ttrain-rmse:0.522471+0.00727149\ttest-rmse:0.539523+0.10686\n",
      "[74]\ttrain-rmse:0.511824+0.00741687\ttest-rmse:0.529709+0.105793\n",
      "[75]\ttrain-rmse:0.501666+0.0074071\ttest-rmse:0.519732+0.105981\n",
      "[76]\ttrain-rmse:0.492443+0.00767088\ttest-rmse:0.512146+0.104744\n",
      "[77]\ttrain-rmse:0.48397+0.00768751\ttest-rmse:0.504754+0.103955\n",
      "[78]\ttrain-rmse:0.476114+0.00794273\ttest-rmse:0.497981+0.103285\n",
      "[79]\ttrain-rmse:0.469028+0.00821611\ttest-rmse:0.49204+0.101882\n",
      "[80]\ttrain-rmse:0.46246+0.00818792\ttest-rmse:0.487004+0.101262\n",
      "[81]\ttrain-rmse:0.456002+0.00819204\ttest-rmse:0.481113+0.101078\n",
      "[82]\ttrain-rmse:0.450375+0.00826168\ttest-rmse:0.475864+0.100462\n",
      "[83]\ttrain-rmse:0.445306+0.00838013\ttest-rmse:0.471476+0.0995929\n",
      "[84]\ttrain-rmse:0.44075+0.00854555\ttest-rmse:0.46718+0.0985151\n",
      "[85]\ttrain-rmse:0.436497+0.00850469\ttest-rmse:0.463309+0.0981232\n",
      "[86]\ttrain-rmse:0.432512+0.00872337\ttest-rmse:0.460407+0.0972761\n",
      "[87]\ttrain-rmse:0.428897+0.0087046\ttest-rmse:0.457322+0.0963785\n",
      "[88]\ttrain-rmse:0.4254+0.00879161\ttest-rmse:0.454679+0.0959498\n",
      "[89]\ttrain-rmse:0.422275+0.00873998\ttest-rmse:0.452371+0.0958651\n",
      "[90]\ttrain-rmse:0.419373+0.00895334\ttest-rmse:0.450038+0.0950732\n",
      "[91]\ttrain-rmse:0.41666+0.00905689\ttest-rmse:0.448001+0.094286\n",
      "[92]\ttrain-rmse:0.41419+0.00894872\ttest-rmse:0.445864+0.0941531\n",
      "[93]\ttrain-rmse:0.411909+0.00906078\ttest-rmse:0.444492+0.0932088\n",
      "[94]\ttrain-rmse:0.409723+0.00913751\ttest-rmse:0.44308+0.0926396\n",
      "[95]\ttrain-rmse:0.407918+0.00915561\ttest-rmse:0.441993+0.0927218\n",
      "[96]\ttrain-rmse:0.406221+0.00893886\ttest-rmse:0.439948+0.0927493\n",
      "[97]\ttrain-rmse:0.404535+0.00905188\ttest-rmse:0.438881+0.0929449\n",
      "[98]\ttrain-rmse:0.403086+0.00928303\ttest-rmse:0.437945+0.0928569\n",
      "[99]\ttrain-rmse:0.401739+0.00916705\ttest-rmse:0.437284+0.0925573\n",
      "[100]\ttrain-rmse:0.400557+0.00928263\ttest-rmse:0.436427+0.0925104\n",
      "[101]\ttrain-rmse:0.399371+0.00928023\ttest-rmse:0.435915+0.0922626\n",
      "[102]\ttrain-rmse:0.398173+0.00913459\ttest-rmse:0.435541+0.0921775\n",
      "[103]\ttrain-rmse:0.397268+0.00914227\ttest-rmse:0.434184+0.0916653\n",
      "[104]\ttrain-rmse:0.396126+0.00922255\ttest-rmse:0.434009+0.0917413\n",
      "[105]\ttrain-rmse:0.395224+0.0092993\ttest-rmse:0.433363+0.0911213\n",
      "[106]\ttrain-rmse:0.394267+0.00926812\ttest-rmse:0.433018+0.0911202\n",
      "[107]\ttrain-rmse:0.393384+0.00947578\ttest-rmse:0.43269+0.0910356\n",
      "[108]\ttrain-rmse:0.392538+0.00934747\ttest-rmse:0.431948+0.090934\n",
      "[109]\ttrain-rmse:0.391538+0.00952003\ttest-rmse:0.431878+0.0912734\n",
      "[110]\ttrain-rmse:0.390806+0.00956956\ttest-rmse:0.431444+0.0909824\n",
      "[111]\ttrain-rmse:0.389946+0.00956773\ttest-rmse:0.431466+0.0909378\n",
      "[112]\ttrain-rmse:0.389057+0.00943985\ttest-rmse:0.431319+0.0906114\n",
      "[113]\ttrain-rmse:0.388389+0.00974113\ttest-rmse:0.430888+0.0908169\n",
      "[114]\ttrain-rmse:0.387878+0.00967878\ttest-rmse:0.431012+0.0905714\n",
      "[115]\ttrain-rmse:0.387019+0.00962742\ttest-rmse:0.430887+0.0903682\n",
      "[116]\ttrain-rmse:0.386372+0.0096245\ttest-rmse:0.430858+0.0902363\n",
      "[117]\ttrain-rmse:0.385877+0.00957672\ttest-rmse:0.43069+0.0902934\n",
      "[118]\ttrain-rmse:0.385074+0.00947072\ttest-rmse:0.430166+0.0904579\n",
      "[119]\ttrain-rmse:0.384379+0.00933904\ttest-rmse:0.42968+0.0899116\n",
      "[120]\ttrain-rmse:0.383953+0.00937439\ttest-rmse:0.429421+0.0899318\n",
      "[121]\ttrain-rmse:0.383244+0.00941147\ttest-rmse:0.429403+0.0905649\n",
      "[122]\ttrain-rmse:0.382812+0.00944275\ttest-rmse:0.428783+0.0908101\n",
      "[123]\ttrain-rmse:0.382406+0.00930211\ttest-rmse:0.429028+0.0905604\n",
      "[124]\ttrain-rmse:0.38202+0.00931819\ttest-rmse:0.429138+0.0904977\n",
      "[125]\ttrain-rmse:0.38149+0.0092915\ttest-rmse:0.4293+0.0904092\n",
      "[126]\ttrain-rmse:0.381195+0.00925895\ttest-rmse:0.429258+0.0903846\n",
      "[127]\ttrain-rmse:0.380841+0.00921804\ttest-rmse:0.429455+0.0903318\n",
      "[128]\ttrain-rmse:0.380366+0.00908845\ttest-rmse:0.42961+0.0902721\n",
      "[129]\ttrain-rmse:0.379855+0.00916082\ttest-rmse:0.429471+0.0902238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130]\ttrain-rmse:0.379352+0.00907348\ttest-rmse:0.429501+0.0900062\n",
      "[131]\ttrain-rmse:0.379062+0.00901647\ttest-rmse:0.429597+0.0897659\n",
      "[132]\ttrain-rmse:0.378781+0.00914285\ttest-rmse:0.42988+0.0898576\n",
      "[133]\ttrain-rmse:0.378378+0.00925128\ttest-rmse:0.429918+0.0900202\n",
      "[134]\ttrain-rmse:0.377993+0.00908776\ttest-rmse:0.429779+0.0895266\n",
      "[135]\ttrain-rmse:0.377622+0.0091181\ttest-rmse:0.429874+0.0896063\n",
      "[136]\ttrain-rmse:0.377161+0.009162\ttest-rmse:0.430053+0.0893568\n",
      "[137]\ttrain-rmse:0.376767+0.00939233\ttest-rmse:0.430207+0.0895385\n",
      "[138]\ttrain-rmse:0.376378+0.00931716\ttest-rmse:0.430181+0.0893658\n",
      "[139]\ttrain-rmse:0.376177+0.00927083\ttest-rmse:0.429402+0.089584\n",
      "[140]\ttrain-rmse:0.375902+0.00915164\ttest-rmse:0.428906+0.0892767\n",
      "[141]\ttrain-rmse:0.375578+0.00916178\ttest-rmse:0.428903+0.0893684\n",
      "[142]\ttrain-rmse:0.375269+0.00920259\ttest-rmse:0.428748+0.0893651\n",
      "[143]\ttrain-rmse:0.375057+0.00915243\ttest-rmse:0.42833+0.0889295\n",
      "[144]\ttrain-rmse:0.374756+0.00926198\ttest-rmse:0.428016+0.0887747\n",
      "[145]\ttrain-rmse:0.374315+0.00918516\ttest-rmse:0.4281+0.0889041\n",
      "[146]\ttrain-rmse:0.37399+0.00934528\ttest-rmse:0.42821+0.0890498\n",
      "[147]\ttrain-rmse:0.373707+0.00930482\ttest-rmse:0.42818+0.0890452\n",
      "[148]\ttrain-rmse:0.373371+0.00941578\ttest-rmse:0.427892+0.0885621\n",
      "[149]\ttrain-rmse:0.3731+0.00952089\ttest-rmse:0.427493+0.0884251\n",
      "[150]\ttrain-rmse:0.372668+0.00974511\ttest-rmse:0.427095+0.0883148\n",
      "[151]\ttrain-rmse:0.372342+0.00967526\ttest-rmse:0.427072+0.0883209\n",
      "[152]\ttrain-rmse:0.372034+0.00964917\ttest-rmse:0.427076+0.0882892\n",
      "[153]\ttrain-rmse:0.371937+0.00961845\ttest-rmse:0.426948+0.0879993\n",
      "[154]\ttrain-rmse:0.37156+0.0097318\ttest-rmse:0.426396+0.0882135\n",
      "[155]\ttrain-rmse:0.37108+0.00975941\ttest-rmse:0.426119+0.0884528\n",
      "[156]\ttrain-rmse:0.370708+0.00961792\ttest-rmse:0.425902+0.0884604\n",
      "[157]\ttrain-rmse:0.370379+0.00966074\ttest-rmse:0.426189+0.0884772\n",
      "[158]\ttrain-rmse:0.370006+0.00971624\ttest-rmse:0.426034+0.0883741\n",
      "[159]\ttrain-rmse:0.369757+0.00971591\ttest-rmse:0.426442+0.0879775\n",
      "[160]\ttrain-rmse:0.369605+0.00973753\ttest-rmse:0.426535+0.0878629\n",
      "[161]\ttrain-rmse:0.36927+0.00976695\ttest-rmse:0.426304+0.0879078\n",
      "[162]\ttrain-rmse:0.368963+0.00964267\ttest-rmse:0.426559+0.0875065\n",
      "[163]\ttrain-rmse:0.368468+0.00971447\ttest-rmse:0.426525+0.0878586\n",
      "[164]\ttrain-rmse:0.368204+0.00955788\ttest-rmse:0.426094+0.087359\n",
      "[165]\ttrain-rmse:0.367813+0.00948591\ttest-rmse:0.425975+0.0872767\n",
      "[166]\ttrain-rmse:0.367487+0.00968361\ttest-rmse:0.426284+0.0878909\n",
      "[167]\ttrain-rmse:0.367096+0.0098486\ttest-rmse:0.426604+0.0879659\n",
      "[168]\ttrain-rmse:0.366631+0.00973911\ttest-rmse:0.426586+0.0878791\n",
      "[169]\ttrain-rmse:0.36627+0.00963936\ttest-rmse:0.426558+0.0880597\n",
      "[170]\ttrain-rmse:0.366029+0.00959189\ttest-rmse:0.42665+0.0881207\n",
      "[171]\ttrain-rmse:0.365672+0.00975981\ttest-rmse:0.42699+0.0887924\n",
      "[172]\ttrain-rmse:0.365437+0.00987831\ttest-rmse:0.427223+0.0889693\n",
      "[173]\ttrain-rmse:0.365092+0.0098219\ttest-rmse:0.427315+0.0888381\n",
      "[174]\ttrain-rmse:0.364701+0.00987117\ttest-rmse:0.427343+0.0887859\n",
      "[175]\ttrain-rmse:0.364529+0.00979073\ttest-rmse:0.427003+0.0889042\n",
      "[176]\ttrain-rmse:0.364347+0.00986186\ttest-rmse:0.427143+0.0889219\n",
      "[177]\ttrain-rmse:0.364053+0.00979934\ttest-rmse:0.427169+0.0889154\n",
      "[178]\ttrain-rmse:0.363778+0.0098076\ttest-rmse:0.427614+0.0890158\n",
      "[179]\ttrain-rmse:0.363562+0.00976166\ttest-rmse:0.42749+0.0888522\n",
      "[180]\ttrain-rmse:0.363301+0.00967137\ttest-rmse:0.427436+0.0888517\n",
      "[181]\ttrain-rmse:0.362904+0.00952226\ttest-rmse:0.427384+0.0886134\n",
      "[182]\ttrain-rmse:0.362589+0.00936209\ttest-rmse:0.427367+0.0885404\n",
      "[183]\ttrain-rmse:0.362316+0.00938496\ttest-rmse:0.42702+0.0891111\n",
      "[184]\ttrain-rmse:0.362043+0.00940131\ttest-rmse:0.426685+0.0893854\n",
      "[185]\ttrain-rmse:0.361885+0.00944905\ttest-rmse:0.426761+0.0893106\n",
      "[186]\ttrain-rmse:0.361427+0.00929478\ttest-rmse:0.426573+0.0892456\n",
      "[187]\ttrain-rmse:0.361244+0.00919196\ttest-rmse:0.427001+0.089149\n",
      "[188]\ttrain-rmse:0.361097+0.00917358\ttest-rmse:0.426768+0.0893455\n",
      "[189]\ttrain-rmse:0.360829+0.00932202\ttest-rmse:0.426705+0.089209\n",
      "[190]\ttrain-rmse:0.360593+0.00923282\ttest-rmse:0.426457+0.0889987\n",
      "[191]\ttrain-rmse:0.360479+0.00921744\ttest-rmse:0.426213+0.0888201\n",
      "[192]\ttrain-rmse:0.360178+0.00931581\ttest-rmse:0.426113+0.0886921\n",
      "[193]\ttrain-rmse:0.359907+0.00940949\ttest-rmse:0.426151+0.0889012\n",
      "[194]\ttrain-rmse:0.359747+0.00939692\ttest-rmse:0.426173+0.0889424\n",
      "[195]\ttrain-rmse:0.359561+0.00932672\ttest-rmse:0.426041+0.0886722\n",
      "[196]\ttrain-rmse:0.359366+0.00929987\ttest-rmse:0.426158+0.0887112\n",
      "[197]\ttrain-rmse:0.359116+0.00929566\ttest-rmse:0.425924+0.0888523\n",
      "[198]\ttrain-rmse:0.358954+0.00923206\ttest-rmse:0.425979+0.0887856\n",
      "[199]\ttrain-rmse:0.358715+0.00922098\ttest-rmse:0.425667+0.0888468\n",
      "[200]\ttrain-rmse:0.358562+0.00934846\ttest-rmse:0.425897+0.088927\n",
      "[201]\ttrain-rmse:0.358485+0.00934598\ttest-rmse:0.42622+0.0887813\n",
      "[202]\ttrain-rmse:0.358239+0.0093344\ttest-rmse:0.426146+0.0886948\n",
      "[203]\ttrain-rmse:0.35796+0.00933313\ttest-rmse:0.426144+0.0887096\n",
      "[204]\ttrain-rmse:0.357681+0.0095163\ttest-rmse:0.426272+0.0886978\n",
      "[205]\ttrain-rmse:0.357436+0.00956061\ttest-rmse:0.426203+0.0888893\n",
      "[206]\ttrain-rmse:0.357217+0.00963802\ttest-rmse:0.42614+0.0889076\n",
      "[207]\ttrain-rmse:0.35699+0.00956035\ttest-rmse:0.426211+0.0886392\n",
      "[208]\ttrain-rmse:0.356709+0.0095994\ttest-rmse:0.426296+0.0885716\n",
      "[209]\ttrain-rmse:0.356462+0.00951562\ttest-rmse:0.426124+0.088739\n",
      "[210]\ttrain-rmse:0.356163+0.00959469\ttest-rmse:0.425944+0.0886438\n",
      "[211]\ttrain-rmse:0.355925+0.0096521\ttest-rmse:0.42626+0.0883951\n",
      "[212]\ttrain-rmse:0.355673+0.00972009\ttest-rmse:0.426311+0.0885961\n",
      "[213]\ttrain-rmse:0.355492+0.00973467\ttest-rmse:0.42643+0.088615\n",
      "[214]\ttrain-rmse:0.355378+0.00968686\ttest-rmse:0.426513+0.0885171\n",
      "[215]\ttrain-rmse:0.355198+0.00969965\ttest-rmse:0.426827+0.0881646\n",
      "[216]\ttrain-rmse:0.355035+0.00964827\ttest-rmse:0.426661+0.0882756\n",
      "[217]\ttrain-rmse:0.354787+0.00964483\ttest-rmse:0.426457+0.0884447\n",
      "[218]\ttrain-rmse:0.354599+0.00971344\ttest-rmse:0.426475+0.0884778\n",
      "[219]\ttrain-rmse:0.354477+0.00979136\ttest-rmse:0.426572+0.0887078\n",
      "[220]\ttrain-rmse:0.354174+0.0097994\ttest-rmse:0.426628+0.0887499\n",
      "[221]\ttrain-rmse:0.354101+0.009796\ttest-rmse:0.426521+0.0887305\n",
      "[222]\ttrain-rmse:0.35381+0.00976902\ttest-rmse:0.42656+0.0884233\n",
      "[223]\ttrain-rmse:0.353541+0.00974322\ttest-rmse:0.426616+0.0887318\n",
      "[224]\ttrain-rmse:0.353348+0.00968776\ttest-rmse:0.426863+0.0887025\n",
      "[225]\ttrain-rmse:0.35323+0.00964725\ttest-rmse:0.426925+0.0888647\n",
      "[226]\ttrain-rmse:0.353071+0.00964963\ttest-rmse:0.426729+0.0890002\n",
      "[227]\ttrain-rmse:0.352864+0.00966983\ttest-rmse:0.426636+0.0891497\n",
      "[228]\ttrain-rmse:0.352676+0.0096676\ttest-rmse:0.427163+0.0891631\n",
      "[229]\ttrain-rmse:0.352555+0.00970576\ttest-rmse:0.427046+0.0889225\n",
      "[230]\ttrain-rmse:0.352422+0.00970391\ttest-rmse:0.426996+0.0888692\n",
      "[231]\ttrain-rmse:0.352162+0.00967098\ttest-rmse:0.42744+0.0884299\n",
      "[232]\ttrain-rmse:0.351986+0.00965366\ttest-rmse:0.427388+0.0884373\n",
      "[233]\ttrain-rmse:0.351817+0.00956742\ttest-rmse:0.427537+0.0885396\n",
      "[234]\ttrain-rmse:0.351611+0.00950464\ttest-rmse:0.427437+0.0887543\n",
      "[235]\ttrain-rmse:0.351326+0.0095345\ttest-rmse:0.427503+0.0886653\n",
      "[236]\ttrain-rmse:0.35117+0.00952713\ttest-rmse:0.427308+0.0886519\n",
      "[237]\ttrain-rmse:0.350851+0.00966892\ttest-rmse:0.427149+0.0885635\n",
      "[238]\ttrain-rmse:0.35065+0.00966435\ttest-rmse:0.426766+0.0882738\n",
      "[239]\ttrain-rmse:0.350454+0.00977999\ttest-rmse:0.426906+0.0883667\n",
      "[240]\ttrain-rmse:0.350311+0.00980586\ttest-rmse:0.427321+0.0882872\n",
      "[241]\ttrain-rmse:0.350092+0.00976425\ttest-rmse:0.427117+0.0882397\n",
      "[242]\ttrain-rmse:0.349811+0.00975824\ttest-rmse:0.42754+0.0877854\n",
      "[243]\ttrain-rmse:0.349681+0.00972284\ttest-rmse:0.427778+0.0876169\n",
      "[244]\ttrain-rmse:0.349408+0.00967126\ttest-rmse:0.427323+0.0875096\n",
      "[245]\ttrain-rmse:0.349297+0.00963138\ttest-rmse:0.427181+0.0874616\n",
      "[246]\ttrain-rmse:0.348999+0.00958967\ttest-rmse:0.427089+0.0875246\n",
      "[247]\ttrain-rmse:0.348781+0.00965619\ttest-rmse:0.426845+0.0876567\n",
      "[248]\ttrain-rmse:0.348549+0.00971634\ttest-rmse:0.426904+0.0877308\n"
     ]
    }
   ],
   "source": [
    "xgbCV = xgb.cv(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 10, #same as CV\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalise XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "xgb.plot_importance(xgbFinal, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = best_rf.predict(test_rf_x_engine)\n",
    "xgbFinal_submission = np.exp(xgbFinal_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = xgbFinal.predict(xgb.DMatrix(test_rf_x_engine))\n",
    "xgbFinal_submission = np.exp(xgbFinal_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other submission style\n",
    "## Creating a Submission File to submit to Kaggle competition ##\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": testData[\"Id\"],\n",
    "        \"Prediction\": xgbFinal_submission\n",
    "    })\n",
    "submission.to_csv('Best_model_2(0.05)_trial.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = pickle.load(open(\"xgbFinal.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-forecast the prediction to verify the model\n",
    "xgb_test_p = xgb_test.predict(xgb.DMatrix(test_rf_x))\n",
    "xgb_test_p = np.exp(xgb_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = (xgbFinal_submission == xgb_test_p)\n",
    "verify_model.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6th, XGBoost, Grid Search with Mutual Features & High Correlation(0.15) with sqrt_log_transformation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\"learning_rate\": [0.1,0.2,0.3],              # range: [0,1], default = 0.3\n",
    "                   # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "                   # The larger gamma is, the more conservative the algorithm will be.                   \n",
    "                   \"gamma\": [i/10.0 for i in range(1,10,2)],\n",
    "                   # Maximum depth of a tree. \n",
    "                   # Increasing this value will make the model more complex and more likely to overfit\n",
    "                   \"max_depth\": [6], \n",
    "                   # Used to control over-fitting\n",
    "                   # Too high values can lead to under-fitting hence, it should be tuned using grid search\n",
    "                   \"min_child_weight\": [1],\n",
    "                   # Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced\n",
    "                   \"max_delta_step\": [0],\n",
    "                   # Denotes the fraction of observations to be randomly samples for each tree\n",
    "                   # Typical values: 0.5-1\n",
    "                   \"subsample\": [0.8],\n",
    "                   # Similar to max_features in GBM, Typical values: 0.5 - 1\n",
    "                   # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "                   \"colsample_bytree\": [0.8,1],\n",
    "                   \"colsample_bylevel\": [1], # colsample_bytree will control the decision over this, default = 1\n",
    "                   \"colsample_bynode\": [1],  # colsample_bytree will control the decision over this, default = 1\n",
    "                   # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_lambda\": [1], # default = 1\n",
    "                   # L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "                   \"reg_alpha\": [1],  # default = 0\n",
    "                   # Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "                   # A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "                   \"scale_pos_weight\": [1],\n",
    "                   # No. of trees ensemble, too high sometimes still can cause overfitting\n",
    "                   \"n_estimators\": [300,400], \n",
    "                   \"booster\": [\"gbtree\"],\n",
    "                   \"verbosity\": [1],\n",
    "                   \"objective\": [\"reg:squarederror\"],\n",
    "                   \"seed\": [50]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the iteration records for parameters tuning\n",
    "best_score_list = []\n",
    "best_params_list = []\n",
    "best_R2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_tuning = {'learning_rate': [0.05],               # 6th when boosting\n",
    "                          'gamma': [0.1],                       # 3rd\n",
    "                          'max_depth': [3],                     # 1st to tune\n",
    "                          'min_child_weight': [0.1],              # 2nd\n",
    "                          'max_delta_step': [0],\n",
    "                          'subsample': [0.9],                   # 4th\n",
    "                          'colsample_bytree': [0.2],            # 4th \n",
    "                          'colsample_bylevel': [1],\n",
    "                          'colsample_bynode': [1],\n",
    "                          'reg_lambda': [0],                    # 5th\n",
    "                          'reg_alpha': [0],                     # 5th\n",
    "                          'scale_pos_weight': [1.0],            # only when dealing with imbalance classes\n",
    "                          'n_estimators': [130],                # 1st\n",
    "                          \"booster\": [\"gbtree\"],\n",
    "                          \"verbosity\": [1],\n",
    "                          \"objective\": [\"reg:squarederror\"],\n",
    "                          \"seed\": [99]\n",
    "                         }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Parameters setting inside Regressor\n",
    "xgboost = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid = GridSearchCV(estimator = xgboost, param_grid = hyperparameters_tuning, cv = 10, iid = False, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_grid.fit(train_rf_x_engine,sqrt_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_score = xgboost_grid.cv_results_ #thus no need train-test split, as cv will automatic run for us \n",
    "best_params = xgboost_grid.best_params_\n",
    "best_score = xgboost_grid.best_score_\n",
    "best_rf = xgboost_grid.best_estimator_\n",
    "best_R2_score = best_rf.score(train_rf_x_engine,sqrt_log_y)\n",
    "best_score_list.append(best_score)\n",
    "best_params_list.append(best_rf)\n",
    "best_R2_list.append(best_R2_score)\n",
    "print(best_score)\n",
    "print(best_R2_score)\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_score_list)\n",
    "print(best_R2_list)\n",
    "print(best_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost's DMatrix, after fine tuning the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = train_rf_x_engine, label = sqrt_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the learning_rate and set a large num_boost_round hyperparameter to ensure convergence. \n",
    "# If convergence is slow, retry with a slightly higher learning rate (e.g. 0.075 instead of 0.05)\n",
    "num_boost_round = 150000\n",
    "early_stopping_rounds = 50\n",
    "# Activates early stopping. CV error needs to decrease at least every <early_stopping_rounds> round(s) to continue.\n",
    "# Last entry in evaluation history is the one from best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_boosting = {'learning_rate': 0.001,               # 6th when boosting\n",
    "                          'gamma': 0.1,                       # 3rd\n",
    "                          'max_depth': 3,                     # 1st to tune\n",
    "                          'min_child_weight': 0.1,              # 2nd\n",
    "                          'max_delta_step': 0,\n",
    "                          'subsample': 0.9,                   # 4th\n",
    "                          'colsample_bytree': 0.2,            # 4th \n",
    "                          'colsample_bylevel': 1,\n",
    "                          'colsample_bynode': 1,\n",
    "                          'reg_lambda': 0,                    # 5th\n",
    "                          'reg_alpha': 0,                     # 5th\n",
    "                          'scale_pos_weight': 1.0,            # only when dealing with imbalance classes\n",
    "                          'n_estimators': 130,                # 1st\n",
    "                          \"booster\": \"gbtree\",\n",
    "                          \"verbosity\": 1,\n",
    "                          \"objective\": \"reg:squarederror\",\n",
    "                          \"seed\": 99\n",
    "                         }                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbCV = xgb.cv(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 10, #same as CV\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalise XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = hyperparameters_boosting, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "xgb.plot_importance(xgbFinal, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_submission = xgbFinal.predict(xgb.DMatrix(test_rf_x_engine))\n",
    "xgbFinal_submission = np.square(np.exp(xgbFinal_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other submission style\n",
    "## Creating a Submission File to submit to Kaggle competition ##\n",
    "testData = pd.read_csv(\"test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": testData[\"Id\"],\n",
    "        \"Prediction\": xgbFinal_submission\n",
    "    })\n",
    "submission.to_csv('Best_model_1(0.001)_trial.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test = pickle.load(open(\"xgbFinal.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-forecast the prediction to verify the model\n",
    "xgb_test_p = xgb_test.predict(xgb.DMatrix(test_rf_x_engine))\n",
    "xgb_test_p = np.exp(xgb_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_model = (xgbFinal_submission == xgb_test_p)\n",
    "verify_model.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
